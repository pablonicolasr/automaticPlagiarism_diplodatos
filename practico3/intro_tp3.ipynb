{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import operator\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.colors as mc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import textstat\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import math\n",
    "import textstat\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "\n",
    "#nltk.install(\"all\")\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from seg import Segmentation\n",
    "\n",
    " # Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# os.remove(path)\n",
    "# if os.path.exists(path):\n",
    "#    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1968af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct 10 19:16:37 2020\n",
    "\n",
    "@author: pablonicolasr\n",
    "\"\"\"\n",
    "from dataset import generate\n",
    "from cluster import cluster_ip\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "actual_directory = os.getcwd()\n",
    "\n",
    "ruta = os.path.join(actual_directory, \"mini_corpus\")\n",
    "ruta_del_directorio = str(ruta) + \"\\\\*.txt\"\n",
    "# Me produce una lista con los nombres de los archivos txt del corpus\n",
    "textos_originales = glob.glob(ruta_del_directorio)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\cleaned_corpus\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\cleaned_corpus')\n",
    "    \n",
    "cleaned_corpus = str(actual_directory)+\"\\\\cleaned_corpus\"\n",
    "\n",
    "# De esta manera le voy robo el nombre a los xml\n",
    "path_xml = str(actual_directory) + \"\\\\mini_corpus\" + \"\\\\*.xml\"\n",
    "xml_docs = glob.glob(path_xml)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\sospechoso\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\sospechoso')\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos\")\n",
    "    patron_inicio = 'sosp*'\n",
    "    patron_final = '*.txt'\n",
    "    archivos = os.listdir(str(actual_directory)+\"\\\\sospechoso\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_inicio) and fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory)+\"\\\\sospechoso\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "    \n",
    "if not os.path.exists(str(actual_directory) + \"\\\\xml\\\\output_xml\"):\n",
    "    os.mkdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos en formato xml\")\n",
    "    patron_final = '*.xml'\n",
    "    archivos = os.listdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory) + \"\\\\xml\\\\output_xml\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "\n",
    "sospechosos_path = str(actual_directory) + \"\\\\sospechoso\\\\\"\n",
    "output_xml = str(actual_directory) + \"\\\\xml\\\\output_xml\\\\\"\n",
    "\n",
    "\n",
    "f = {}\n",
    "# Comienzo a leer el conjunto de textos\n",
    "for i in range(len(textos_originales)):\n",
    "    print(\"File: \" + str(i+1))\n",
    "    f[i] = open(textos_originales[i], encoding= \"utf-8-sig\")\n",
    "    text = f[i].read()\n",
    "    f[i].close()            \n",
    "    \n",
    "    # Nombre de Archivo\n",
    "    filename = os.path.basename(textos_originales[i])\n",
    "    # Separo nombre de archivo y extension\n",
    "    filename2, extension = os.path.splitext(filename)\n",
    "    \n",
    "    Seg = Segmentation(text)\n",
    "    # La variable data_text posee el texto completo, separado en párrafos\n",
    "    data_text = Seg.paraSegmentation()\n",
    "    data_list = []\n",
    "    length = []\n",
    "    \n",
    "    # Se tiene guardar el texto sin limpiar\n",
    "    for segmento in data_text:\n",
    "        # A cada segmento se le borra el salto de línea\n",
    "        segmento = re.sub(\"\\n\", \" \", segmento)\n",
    "        # Se guarda la longitud del segmento\n",
    "        longitud = len(segmento)\n",
    "        length.append(longitud)\n",
    "        # Se guarda el segmento\n",
    "        data_list.append(segmento)\n",
    "    \n",
    "    # De aqui empezar a procesar la limpieza de texto \n",
    "    # Some code...\n",
    "    # .....\n",
    "    # .....\n",
    "    \n",
    "    rootPath = ruta + \"\\\\\"+ filename\n",
    "    outputPath = str(actual_directory) + \"\\\\csv\\\\\"+ filename2 + \".csv\"\n",
    "    \n",
    "    if os.path.exists(str(actual_directory) + \"\\\\csv\\\\\"+ filename2 + \".csv\"):\n",
    "        os.remove(str(actual_directory) + \"\\\\csv\\\\\"+ filename2 + \".csv\")\n",
    "    \n",
    "    generate(rootPath, outputPath, filename, \"english\")\n",
    "    \n",
    "    df = pd.read_csv(outputPath)\n",
    "    \n",
    "    # Jugar con estos números // Tarea\n",
    "    \n",
    "    # samples = len(length) * 0.075\n",
    "    \n",
    "    # decimal, entero = math.modf(samples)\n",
    "    \n",
    "    # if decimal >= 0.5:\n",
    "        # min_samples = entero + 1\n",
    "    # else:\n",
    "        # min_samples = entero\n",
    "    \n",
    "    # Este número fue sacado de un paper\n",
    "    min_samples = 4\n",
    "    \n",
    "    # Normalizar de forma correcta y eficiente el indice de facilidad de lectura\n",
    "    df[\"fleshReadingEase\"] = df[\"fleshReadingEase\"] / df[\"fleshReadingEase\"].max()\n",
    "    \n",
    "    # Seleccionar los valores numéricos del df\n",
    "    # Este paso se realiza para ir viendo que genera cada párrafo del txt y como fue convertido a un vector\n",
    "    dataframe = df.iloc[:,[2,3,4]]\n",
    "    \n",
    "    # Aprendizaje No Supervisado\n",
    "    cluster = cluster_ip(dataframe, min_samples, \"cosine\")\n",
    "    labels = cluster.labels_    \n",
    "    df[\"labels\"] = labels\n",
    "    \n",
    "    realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clusterNum=len(set(labels))\n",
    "    n_noise = list(labels).count(-1) \n",
    "    \n",
    "    print(\"Estimated number of clusters: %d\" % realClusterNum)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "    counts = df[\"labels\"].value_counts().to_dict()\n",
    "    print(counts)\n",
    "    \n",
    "    # Me sirve para guardar los casos de plagio y compararlos\n",
    "    xml_name = os.path.basename(xml_docs[i])\n",
    "    xml_name2, extension = os.path.splitext(xml_name)\n",
    "    \n",
    "    # Se crea el archivo txt donde se guardaran los fragmentos sospechosos\n",
    "    sherlock = codecs.open(sospechosos_path + \"sosp\" + filename,\"w\",\"utf-8\")\n",
    "    \n",
    "    # Los fragmentos que fueron clasificados como plagio\n",
    "    # Se guardan en el archivo .txt\n",
    "    # Tener en cuenta que el dataframe y la lista data_list, manejan los mismos índices\n",
    "    # Por eso, si en el dataframe se encuentra un \"-1\" es porque este es un outlier\n",
    "    for x in range(len(df[\"labels\"])):\n",
    "        l = df[\"labels\"][x]\n",
    "        if l == -1:\n",
    "            print(data_list[x])\n",
    "            print(x)\n",
    "            sherlock.write(data_list[x]+os.linesep)       \n",
    "               \n",
    "    sherlock.close()    \n",
    "    \n",
    "    document_el = etree.Element(\"document\", reference = xml_name2 + \".txt\")\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for y in range(len(df[\"labels\"])):       \n",
    "        \n",
    "        l = df[\"labels\"][y]\n",
    "        \n",
    "        if l == -1: \n",
    "            \n",
    "            feature_el = etree.SubElement(document_el, \"feature\", name=\"detected-plagiarism\", this_offset=str(offset), this_length=str(length[y]))\n",
    "    \n",
    "        offset += length[y] + 1\n",
    "                          \n",
    "            \n",
    "    et = etree.ElementTree(document_el)\n",
    "    with open(output_xml+xml_name, \"wb\") as fe:\n",
    "        et.write(fe, encoding=\"utf-8\", xml_declaration=None, pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148148ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738a509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d57b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
