{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84d4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import operator\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.colors as mc\n",
    "import nltk\n",
    "import numpy as np\n",
    "import textstat\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import math\n",
    "import textstat\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "\n",
    "#nltk.install(\"all\")\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nlp\n",
    "\n",
    "from seg import Segmentation\n",
    "\n",
    " # Feature Selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# os.remove(path)\n",
    "# if os.path.exists(path):\n",
    "#    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1968af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se elimino carpeta vieja de fragmentos sospechosos\n",
      "Se elimino carpeta vieja de fragmentos sospechosos en formato xml\n",
      "File: 1\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 2\n",
      "{0: 60, -1: 2}\n",
      "The New Orleans riot agitated the whole country, and the official and other reports served to intensify and concentrate the opposition to President Johnson's policy of reconstruction, a policy resting exclusively on and inspired solely by the executive authority--for it was made plain, by his language and his acts, that he was seeking to rehabilitate the seceded States under conditions differing not a whit from those existing before the rebellion; that is to say, without the slightest constitutional provision regarding the status of the emancipated slaves, and with no assurances of protection for men who had remained loyal in the war.\n",
      "29\n",
      "A copy of the order embodying the Reconstruction law, together with my assignment, having reached me a few days after, I regularly assumed control of the Fifth Military District on March 19, by an order wherein I declared the State and municipal governments of the district to be provisional only, and, under the provisions of the sixth section of the Act, subject to be controlled, modified, superseded, or abolished.  I also announced that no removals from office would be made unless the incumbents failed to carry out the provisions of the law or impeded reorganization, or unless willful delays should necessitate a change, and added: \"Pending the reorganization, it is, desirable and intended to create as little disturbance in the machinery of the various branches of the provisional governments as possible, consistent with the law of Congress and its successful execution, but this condition is dependent upon the disposition shown by the people, and upon the length of time required for reorganization.\"\n",
      "38\n",
      "29\n",
      "Case 1, y == (last + 1)\n",
      "38\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "Entro2\n",
      "File: 2\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 3\n",
      "{0: 471, -1: 3}\n",
      "1873\n",
      "8\n",
      "INTRODUCTION.\n",
      "43\n",
      "* * * * *\n",
      "267\n",
      "8\n",
      "Case 1, y == (last + 1)\n",
      "43\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "267\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "Entro2\n",
      "File: 3\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 85}\n",
      "File: 4\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 56}\n",
      "File: 5\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 4\n",
      "{0: 65, -1: 4}\n",
      "Nagasaki\n",
      "7\n",
      "GENERAL COMPARISON OF HIROSHIMA AND NAGASAKI\n",
      "17\n",
      "  Destruction of Buildings and Houses          Number     Percentage   (Compiled by Nagasaki Municipality)\n",
      "55\n",
      "As intended, the bomb was exploded at an almost ideal location over Nagasaki to do the maximum damage to industry, including the Mitsubishi Steel and Arms Works, the Mitsubishi-Urakami Ordnance Works (Torpedo Works), and numerous factories, factory training schools, and other industrial establishments, with a minimum destruction of dwellings and consequently, a minimum amount of casualties.  Had the bomb been dropped farther south, the Mitsubishi-Urakami Ordnance Works would not have been so severely damaged, but the main business and residential districts of Nagasaki would have sustained much greater damage casualties.\n",
      "63\n",
      "7\n",
      "Case 1, y == (last + 1)\n",
      "17\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "55\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "63\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "Entro2\n",
      "File: 6\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 365}\n",
      "File: 7\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 0\n",
      "{0: 7}\n",
      "File: 8\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 2\n",
      "{0: 1546, -1: 2}\n",
      "* * * * *\n",
      "988\n",
      "* * * * *\n",
      "990\n",
      "988\n",
      "Case 1, y == (last + 1)\n",
      "990\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "Entro2\n",
      "File: 9\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 1\n",
      "{0: 10, -1: 1}\n",
      "Many other improbabilities might be added to the list, and will be found in the complete edition of that history, from which some extracts will be presently given, and which has been published (under the title of \"Historic Certainties\") by Aristarchus Newlight, with a learned commentary (not, indeed, adopting the views contained in these pages, but) quite equal in ingenuity to a late work on the \"Hebrew Monarchy.\" \n",
      "7\n",
      "7\n",
      "Case 1, y == (last + 1)\n",
      "Entro2\n",
      "File: 10\n",
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 2\n",
      "{0: 825, -1: 2}\n",
      "INTRODUCTION\n",
      "15\n",
      "INTRODUCTION.\n",
      "197\n",
      "15\n",
      "Case 1, y == (last + 1)\n",
      "197\n",
      "[]\n",
      "Case 2, y != (last + 1)\n",
      "Entro2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct 10 19:16:37 2020\n",
    "\n",
    "@author: pablonicolasr\n",
    "\"\"\"\n",
    "from dataset import generate\n",
    "from cluster import cluster_ip\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "actual_directory = os.getcwd()\n",
    "\n",
    "ruta = os.path.join(actual_directory, \"mini_corpus\")\n",
    "ruta_del_directorio = str(ruta) + \"\\\\*.txt\"\n",
    "# Me produce una lista con los nombres de los archivos txt del corpus\n",
    "textos_originales = glob.glob(ruta_del_directorio)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\cleaned_corpus\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\cleaned_corpus')\n",
    "    \n",
    "cleaned_corpus = str(actual_directory)+\"\\\\cleaned_corpus\"\n",
    "\n",
    "# De esta manera le voy robo el nombre a los xml\n",
    "path_xml = str(actual_directory) + \"\\\\mini_corpus\" + \"\\\\*.xml\"\n",
    "xml_docs = glob.glob(path_xml)\n",
    "\n",
    "\n",
    "if not os.path.exists(str(actual_directory)+\"\\\\sospechoso\"):\n",
    "    os.mkdir(str(actual_directory)+'\\\\sospechoso')\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos\")\n",
    "    patron_inicio = 'sosp*'\n",
    "    patron_final = '*.txt'\n",
    "    archivos = os.listdir(str(actual_directory)+\"\\\\sospechoso\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_inicio) and fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory)+\"\\\\sospechoso\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "    \n",
    "if not os.path.exists(str(actual_directory) + \"\\\\xml\\\\output_xml\"):\n",
    "    os.mkdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "else:\n",
    "    print(\"Se elimino carpeta vieja de fragmentos sospechosos en formato xml\")\n",
    "    patron_final = '*.xml'\n",
    "    archivos = os.listdir(str(actual_directory) + \"\\\\xml\\\\output_xml\")\n",
    "    for archivo in archivos:\n",
    "        if fnmatch.fnmatch(archivo, patron_final):\n",
    "            ruta_completa = os.path.join(str(actual_directory) + \"\\\\xml\\\\output_xml\", archivo)\n",
    "            os.remove(ruta_completa)\n",
    "            print(f\"Eliminado: {ruta_completa}\")\n",
    "\n",
    "\n",
    "sospechosos_path = str(actual_directory) + \"\\\\sospechoso\\\\\"\n",
    "output_xml = str(actual_directory) + \"\\\\xml\\\\output_xml\\\\\"\n",
    "\n",
    "\n",
    "# Comienzo a leer el conjunto de textos\n",
    "for i, archive in enumerate(textos_originales):\n",
    "    \n",
    "    print(\"File: \" + str(i+1))\n",
    "    \n",
    "    f = open(archive, encoding=\"utf-8-sig\")\n",
    "    # Leo contenido del archivo\n",
    "    text = f.read()\n",
    "\n",
    "    # Cierro el txt\n",
    "    f.close()           \n",
    "    \n",
    "    # Nombre de Archivo\n",
    "    filename = os.path.basename(archive)\n",
    "    # Separo nombre de archivo y extension\n",
    "    filename2, extension = os.path.splitext(filename)\n",
    "    \n",
    "    Seg = Segmentation(text)\n",
    "    # La variable data_text posee el texto completo, separado en párrafos\n",
    "    data_text = Seg.paraSegmentation()\n",
    "    \n",
    "    data_list = []\n",
    "\n",
    "    lista_to_df = []\n",
    "    \n",
    "    for j, segmento in enumerate(data_text):\n",
    "\n",
    "        data_list.append([j, re.sub(\"\\n\", \" \", segmento), len(re.sub(\"\\n\", \" \", segmento))])\n",
    "\n",
    "        lista_to_df.append(\n",
    "            [j, filename, nlp.getnumOfPunctN(segmento), nlp.gettypeToken(segmento)]\n",
    "        )                        \n",
    "    \n",
    "    df = pd.DataFrame(lista_to_df, columns=[\"index\", \"filename\", \"getnumOfPunctN\", \"gettypeToken\"])\n",
    "    \n",
    "    \n",
    "    #df[\"getfleshReadingEase\"] = df[\"getfleshReadingEase\"] / df[\"getfleshReadingEase\"].max()\n",
    "    \n",
    "    # Seleccionar los valores numéricos del df\n",
    "    # Este paso se realiza para ir viendo que genera cada párrafo del txt y como fue convertido a un vector\n",
    "    dataframe = df.iloc[:,[2,3]]\n",
    "    \n",
    "    # Aprendizaje No Supervisado\n",
    "    cluster = cluster_ip(dataframe, 4, \"cosine\")\n",
    "    labels = cluster.labels_    \n",
    "    df[\"labels\"] = labels\n",
    "    \n",
    "    realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clusterNum=len(set(labels))\n",
    "    n_noise = list(labels).count(-1) \n",
    "    \n",
    "    print(\"Estimated number of clusters: %d\" % realClusterNum)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "    counts = df[\"labels\"].value_counts().to_dict()\n",
    "    print(counts)\n",
    "    \n",
    "    # Me sirve para guardar los casos de plagio y compararlos\n",
    "    xml_name = os.path.basename(xml_docs[i])\n",
    "    xml_name2, extension = os.path.splitext(xml_name)\n",
    "    \n",
    "    # Se crea el archivo txt donde se guardaran los fragmentos sospechosos\n",
    "    sherlock = codecs.open(sospechosos_path + \"sosp\" + filename,\"w\",\"utf-8\")\n",
    "    \n",
    "    # Los fragmentos que fueron clasificados como plagio\n",
    "    # Se guardan en el archivo .txt\n",
    "    # Tener en cuenta que el dataframe y la lista data_list, manejan los mismos índices\n",
    "    # Por eso, si en el dataframe se encuentra un \"-1\" es porque este es un outlier\n",
    "    for x in range(len(df[\"labels\"])):\n",
    "        l = df[\"labels\"][x]\n",
    "        if l == -1:\n",
    "            print(data_list[x][1])\n",
    "            print(x)\n",
    "            sherlock.write(data_list[x][1]+os.linesep)       \n",
    "               \n",
    "    sherlock.close()    \n",
    "    \n",
    "    document_el = etree.Element(\"document\", reference = xml_name2 + \".txt\")\n",
    "\n",
    "    offset = []\n",
    "\n",
    "    init = 0\n",
    "\n",
    "    sum_length = []\n",
    "\n",
    "    last = -1\n",
    "\n",
    "    for y in range(len(df[\"labels\"])):       \n",
    "\n",
    "        l = df[\"labels\"][y]\n",
    "\n",
    "        if l == -1:\n",
    "\n",
    "            print(y)\n",
    "\n",
    "            if y == 0:\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 0, y = 0\")\n",
    "\n",
    "            elif y != 0 and (y == (last + 1) or len(sum_length) == 0):\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 1, y == (last + 1)\")\n",
    "\n",
    "            elif y != 0 and y != (last + 1):\n",
    "\n",
    "                this_length = sum([l[1] for l in sum_length])\n",
    "\n",
    "                feature_el = etree.SubElement(document_el, \"feature\", name=\"detected-plagiarism\", this_offset=str(sum_length[0][0]), this_length=str(this_length))\n",
    "\n",
    "                sum_length.clear()\n",
    "\n",
    "                print(sum_length)\n",
    "\n",
    "                sum_length.append([init, data_list[y][2]])\n",
    "\n",
    "                print(f\"Case 2, y != (last + 1)\")   \n",
    "\n",
    "            last = y\n",
    "\n",
    "        if y == (len(df[\"labels\"]) - 1) and len(sum_length) > 0:\n",
    "\n",
    "            print(\"Entro2\")\n",
    "\n",
    "            this_length = sum([l[1] for l in sum_length])\n",
    "\n",
    "            feature_el = etree.SubElement(document_el, \"feature\", name=\"detected-plagiarism\", this_offset=str(sum_length[0][0]), this_length=str(this_length))\n",
    "\n",
    "\n",
    "        init += data_list[y][2] + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    et = etree.ElementTree(document_el)\n",
    "    with open(output_xml+xml_name, \"wb\") as fe:\n",
    "        et.write(fe, encoding=\"utf-8\", xml_declaration=None, pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148148ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738a509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d57b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
